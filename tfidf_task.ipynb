{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Real-World Applications: TF-IDF\n",
    "\n",
    "In this task Hadoop Streaming is used to process Wikipedia articles dump (/data/wiki/en_articles_part).\n",
    "\n",
    "The purpose of this task is to calculate tf*idf for each pair (word, article) from the Wikipedia dump. Apply the stop words filter to speed up calculations. Term frequency (tf) is a function depending on a term (word) and a document (article):\n",
    "\n",
    "tf(term, doc_id) = Nt/N,\n",
    "\n",
    "where Nt - quantity of particular term in the document, N - the total number of terms in the document (without stop words)\n",
    "\n",
    "Inverse document frequency (idf) is a function depends on a term:\n",
    "\n",
    "idf(term) = 1/log(1 + Dt),\n",
    "\n",
    "where Dt - number of documents in the dataset with the particular term.\n",
    "\n",
    "You can find more information here: https://en.wikipedia.xn--org/wiki/Tfidf-q82h but use just the formulas mentioned above.\n",
    "\n",
    "Dataset location: /data/wiki/en_articles_part\n",
    "\n",
    "Stop words list is in ‘/datasets/stop_words_en.txt’ file.\n",
    "\n",
    "Format: article_id article_text\n",
    "\n",
    "To parse the articles don’t forget about Unicode (even though this is an English Wikipedia dump, there are many characters from other languages), remove punctuation marks and transform words to lowercase to get the correct quantities. To cope with Unicode we recommend to use the following tokenizer:\n",
    "\n",
    "Output: tf*idf for term=’labor’ and article_id=12\n",
    "\n",
    "The result on the sample dataset:0.000351\n",
    "\n",
    "Hint: all Wikipedia article_ids are greater than 0. So you can use a dummy article_id=0 to calculate the number of documents with each term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import collections\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8') # required to convert to unicode\n",
    "\n",
    "path = \"stop_words_en.txt\"\n",
    "\n",
    "with open(path, \"r\") as fh:\n",
    "    stop_words = fh.read().splitlines()    \n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        words =  [word.lower().strip() for word in words if (word.lower() not in stop_words)]\n",
    "        \n",
    "        words_counter = collections.Counter(words)\n",
    "        words_total = sum(words_counter.values())\n",
    "\n",
    "        for word, count in sorted(words_counter.items()):\n",
    "            if not word.isalpha(): continue\n",
    "            tf = float(count)/float(words_total)\n",
    "            print(\"{}\\t{}\\t{:f}\".format(word, article_id, tf))\n",
    "             \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "current_word = None\n",
    "tf_memory = dict()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, article_id, tf = line.strip().split('\\t', 2)\n",
    "        tf = float(tf)\n",
    "        \n",
    "        if current_word != word:\n",
    "            if current_word:\n",
    "                idf = float(1)/math.log(1 + article_count)\n",
    "                for article, tff in tf_memory.items():\n",
    "                    print(\"{}\\t{}\\t{:f}\".format(current_word, article, tff*idf))\n",
    "            \n",
    "            current_word = word\n",
    "            article_count = 0\n",
    "            tf_memory.clear()\n",
    "        \n",
    "        article_count += 1\n",
    "        tf_memory[article_id] = float(tf)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "if current_word:\n",
    "    idf = float(1)/math.log(1 + article_count)\n",
    "    for word, tf in tf_memory.items():\n",
    "        print(\"{}\\t{}\\t{:f}\".format(current_word, article, tff*idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/04/21 16:36:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/04/21 16:36:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/04/21 16:36:28 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/04/21 16:36:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/04/21 16:36:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1555864286784_0001\n",
      "19/04/21 16:36:30 INFO impl.YarnClientImpl: Submitted application application_1555864286784_0001\n",
      "19/04/21 16:36:30 INFO mapreduce.Job: The url to track the job: http://6ef687df487d:8088/proxy/application_1555864286784_0001/\n",
      "19/04/21 16:36:30 INFO mapreduce.Job: Running job: job_1555864286784_0001\n",
      "19/04/21 16:36:42 INFO mapreduce.Job: Job job_1555864286784_0001 running in uber mode : false\n",
      "19/04/21 16:36:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/21 16:37:14 INFO mapreduce.Job: Task Id : attempt_1555864286784_0001_m_000001_0, Status : FAILED\n",
      "Container [pid=1666,containerID=container_1555864286784_0001_01_000003] is running beyond virtual memory limits. Current usage: 424.7 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1555864286784_0001_01_000003 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 1679 1666 1666 1666 (java) 654 51 1944674304 54264 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1555864286784_0001/container_1555864286784_0001_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 34115 attempt_1555864286784_0001_m_000001_0 3 \n",
      "\t|- 1666 1659 1666 1666 (bash) 0 0 11550720 187 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1555864286784_0001/container_1555864286784_0001_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 34115 attempt_1555864286784_0001_m_000001_0 3 1>/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000003/stdout 2>/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000003/stderr  \n",
      "\t|- 1746 1679 1666 1666 (java) 0 0 1944674304 54264 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1555864286784_0001/container_1555864286784_0001_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 34115 attempt_1555864286784_0001_m_000001_0 3 \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/04/21 16:37:22 INFO mapreduce.Job:  map 1% reduce 0%\n",
      "19/04/21 16:37:28 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "19/04/21 16:37:34 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "19/04/21 16:37:41 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "19/04/21 16:37:47 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "19/04/21 16:37:49 INFO mapreduce.Job: Task Id : attempt_1555864286784_0001_m_000001_1, Status : FAILED\n",
      "Container [pid=1775,containerID=container_1555864286784_0001_01_000004] is running beyond virtual memory limits. Current usage: 446.6 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1555864286784_0001_01_000004 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 1775 1773 1775 1775 (bash) 0 0 11550720 719 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1555864286784_0001/container_1555864286784_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 34115 attempt_1555864286784_0001_m_000001_1 4 1>/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000004/stdout 2>/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000004/stderr  \n",
      "\t|- 1782 1775 1775 1775 (java) 617 42 1945042944 56809 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1555864286784_0001/container_1555864286784_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 34115 attempt_1555864286784_0001_m_000001_1 4 \n",
      "\t|- 1815 1782 1775 1775 (java) 0 0 1945042944 56809 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1555864286784_0001/container_1555864286784_0001_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1555864286784_0001/container_1555864286784_0001_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 34115 attempt_1555864286784_0001_m_000001_1 4 \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/04/21 16:37:53 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "19/04/21 16:37:59 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "19/04/21 16:38:05 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "19/04/21 16:38:12 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "19/04/21 16:38:18 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "19/04/21 16:38:19 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "19/04/21 16:38:24 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "19/04/21 16:38:25 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "19/04/21 16:38:29 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "19/04/21 16:38:31 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "19/04/21 16:38:35 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "19/04/21 16:38:37 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "19/04/21 16:38:41 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "19/04/21 16:38:43 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "19/04/21 16:38:47 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "19/04/21 16:38:48 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "19/04/21 16:38:53 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "19/04/21 16:38:54 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "19/04/21 16:38:59 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/04/21 16:39:00 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "19/04/21 16:39:05 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "19/04/21 16:39:06 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "19/04/21 16:39:11 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "19/04/21 16:39:12 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "19/04/21 16:39:17 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "19/04/21 16:39:18 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "19/04/21 16:39:23 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "19/04/21 16:39:24 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "19/04/21 16:39:29 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "19/04/21 16:39:30 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "19/04/21 16:39:35 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "19/04/21 16:39:36 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "19/04/21 16:39:41 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "19/04/21 16:39:42 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "19/04/21 16:39:47 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "19/04/21 16:39:48 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "19/04/21 16:39:53 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "19/04/21 16:39:54 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "19/04/21 16:39:59 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "19/04/21 16:40:00 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "19/04/21 16:40:05 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "19/04/21 16:40:06 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "19/04/21 16:40:11 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "19/04/21 16:40:12 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/04/21 16:40:17 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "19/04/21 16:40:18 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "19/04/21 16:40:23 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "19/04/21 16:40:24 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "19/04/21 16:40:29 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "19/04/21 16:40:30 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "19/04/21 16:40:35 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "19/04/21 16:40:36 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "19/04/21 16:40:42 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/04/21 16:40:48 INFO mapreduce.Job:  map 76% reduce 0%\n",
      "19/04/21 16:40:55 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "19/04/21 16:41:01 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "19/04/21 16:41:07 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "19/04/21 16:41:13 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "19/04/21 16:41:14 INFO mapreduce.Job:  map 80% reduce 17%\n",
      "19/04/21 16:41:19 INFO mapreduce.Job:  map 81% reduce 17%\n",
      "19/04/21 16:41:25 INFO mapreduce.Job:  map 82% reduce 17%\n",
      "19/04/21 16:41:31 INFO mapreduce.Job:  map 83% reduce 17%\n",
      "19/04/21 16:41:36 INFO mapreduce.Job:  map 100% reduce 17%\n",
      "19/04/21 16:41:38 INFO mapreduce.Job:  map 100% reduce 37%\n",
      "19/04/21 16:41:44 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "19/04/21 16:41:49 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "19/04/21 16:41:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/04/21 16:41:51 INFO mapreduce.Job: Job job_1555864286784_0001 completed successfully\n",
      "19/04/21 16:41:52 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=72209425\n",
      "\t\tFILE: Number of bytes written=145260532\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=66207255\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=2\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tOther local map tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=513935\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=280000\n",
      "\t\tTotal time spent by all map tasks (ms)=513935\n",
      "\t\tTotal time spent by all reduce tasks (ms)=280000\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=513935\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=280000\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=526269440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=286720000\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=3001073\n",
      "\t\tMap output bytes=66207255\n",
      "\t\tMap output materialized bytes=72209449\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=243185\n",
      "\t\tReduce shuffle bytes=72209449\n",
      "\t\tReduce input records=3001073\n",
      "\t\tReduce output records=3001073\n",
      "\t\tSpilled Records=6002146\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=2533\n",
      "\t\tCPU time spent (ms)=422150\n",
      "\t\tPhysical memory (bytes) snapshot=1180577792\n",
      "\t\tVirtual memory (bytes) snapshot=11798384640\n",
      "\t\tTotal committed heap usage (bytes)=799014912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=66207255\n",
      "19/04/21 16:41:52 INFO streaming.StreamJob: Output directory: tr_idf_1555864583646077\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=\"tr_idf_\"$(date +\"%s%6N\")\n",
    "NUM_REDUCERS=4\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Assignment\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -files mapper.py,reducer.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python mapper.py\" \\\n",
    "    -reducer \"python reducer.py\" \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null \n",
    "\n",
    "hdfs dfs -cat ${OUT_DIR}/part* | grep -w \"labor\" | grep -w \"12\" | cut -f 3\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
